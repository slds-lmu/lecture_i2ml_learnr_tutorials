## Algorithms

### Study Goals

*Theoretical (T)*

- Get familiar with different tuning strategies
- Know the advantages and disadvantages of the tuning strategies

*Practical (P)*

- Learn how to define the search space for parameter tuning.
- Conduct tuning for specific learner

### Preparation

1.  *(T)* Watch the following video:
    <center>
    ![](){width="75%"}
    </center>

1. *(P)* Read the tutorial about [tuning](https://mlr.mlr-org.com/articles/tutorial/tune.html)

### Exercises

#### *(T)* Quiz

```{r tuning-quiz-algorithms, echo=FALSE}
question("Which statements are true?",
  # answer("Tuning means optimizing hyperparameters.", correct = TRUE),
  # answer("Doing tuning well is hard; nested resampling can help.", correct = TRUE),
  # answer("Good tuning is crucial to achieve good performance for all ML algorithms."),
  # # answer("Parallelization is trivial for tuning.", correct = TRUE),
  # answer("Tuning optimizes the inner loss."),
  answer("How well tuning works depends on the learner and the impact of the hyperparameters on that learner.", correct = TRUE),
  answer("Grid search often works better than random search."),
  answer("Grid search scales exponentially with the dimension of the parameter space.", correct = TRUE),
  answer("Grid search evaluates many points from the parameter space that aren't of interest.", correct = TRUE),
  answer("Random search works often better due to it's better exploration of the hyperparameter space.", correct = TRUE),
  answer("Random search scales very well with the dimension of the hyperparameter space."),
  answer("Random search as well as grid search has the problem of discretization.")
)
```

#### *(P)* The sonar task

In this hands-on we want to tune a random forest using the `classif.ranger` learner. The goal is to find hyperparameter values for `mtry` and `min.node.size` to achieve a high AUC. The task we are using is the `sonar.task`. Taking a look at the help page of the task (`??mlbench::Sonar`) gives a nice description:

> This is the data set used by Gorman and Sejnowski in their study of the classification of sonar signals using a neural network [1]. The task is to train a network to discriminate between sonar signals bounced off a metal cylinder and those bounced off a roughly cylindrical rock.
>
> Each pattern is a set of 60 numbers in the range 0.0 to 1.0. Each number represents the energy within a particular frequency band, integrated over a certain period of time. The integration aperture for higher frequencies occur later in time, since these frequencies are transmitted later during the chirp.
>
> The label associated with each record contains the letter "R" if the object is a rock and "M" if it is a mine (metal cylinder). The numbers in the labels are in increasing order of aspect angle, but they do not encode the angle directly.

```{r sonar-task, exercise=TRUE}
sonar.task
```

#### *(P)* Define the learner and parameter set

As mentioned above, we want to find good values for `mtry` and `min.node.size`. Therefore, define a hyperparameter space using `makeParamSet()`. Before defining the parameter set define the `ranger` learner (do also note that we want to optimize for the AUC which requires the estimation of probabilities):

```{r ranger-learner, exercise=TRUE, exercise.checker=learnerChecker("learner", TRUE)}
learner = makeLearner(...)
```

```{r ranger-learner-solution}
# Use the 'ranger.learner' with 'predict.type = "prob"' to prepare the learner for predicting probabilities
learner = makeLearner("classif.ranger", predict.type = "prob")
```

```{r ranger-learner-check}
learner = makeLearner("classif.ranger", predict.type = "prob")
```

Now, define the parameter set with:

- $\texttt{mtry} \in [1,30]$
- $\texttt{min.node.size} \in [1,50]$

```{r tuning-ps, exercise=TRUE, exercise.checker=createChecker("param_set")}
param_set = makeParamSet(
  ..., # mtry
  ...  # min.node.size
)
```

```{r tuning-ps-hint-1}
# To add individual parameter to the space use the make<param.type>Param family.
# To see all available parameter, it's type, the range, and if it is tuneable or not
# with 'getParamSet("classif.ranger")'
getParamSet("classif.ranger")
```

```{r tuning-ps-solution}
param_set = makeParamSet(
  makeIntegerParam("mtry", lower = 1, upper = 30),
  makeIntegerParam("min.node.size", lower = 1, upper = 50)
)
```

```{r tuning-ps-check}
param_set = makeParamSet(
  makeIntegerParam("mtry", lower = 1, upper = 30),
  makeIntegerParam("min.node.size", lower = 1, upper = 50)
)
```

#### *(P)* Choose the tuning and resampling strategy

As tuning strategy use random search with 100 iterations. For the resampling of each value we use a 3-fold cross-validation:

```{r tuning-strategy, exercise=TRUE, exercise.checker=createChecker(c("tune_ctrl", "res_desc"))}
tune_ctrl = ...
res_desc = ...
```

```{r tuning-strategy-hint-1}
# Random search can be defined by calling 'maketunecontrolRandom()'
makeTuneControlRandom(...)
```

```{r tuning-strategy-hint-2}
# to specify the number of iterations (or number of randomly sampled values) use the argument 'maxit'
makeTunecontrolRandom(maxit = 100L)
```

```{r tuning-strategy-hint-3}
# As resampling description use 3-fold cross-validation
res_desc = makeResampleDesc(method = "CV", iters = 3L)
res_desc = cv3
```

```{r tuning-strategy-solution}
tune_ctrl = makeTuneControlRandom(maxit = 100L)
res_desc = makeResampleDesc("CV", iters = 3L)
```

```{r tuning-strategy-check}
tune_ctrl = makeTuneControlRandom(maxit = 100L)
res_desc = makeResampleDesc("CV", iters = 3L)
```

#### *(P)* Run and visualize the tuning

Finally, tune the learner with `tuneParams()`. To do so, you have to collect all previously defined objects and specify the measure we are trying to optimize. **Note** that the tuning could take some time to finish.

```{r, include=FALSE}
tuningChecker = function (label, user_code, check_code, envir_result, evaluate_result, ...)
{
  add_code = "
  df_tune = tune_res$opt.path$env$path
  "

  setup_state(sol_code = paste0(check_code, add_code), stu_code = paste0(user_code, add_code))

  msg = errorToMessage(expr = {
    ex() %>% check_object("learner")
    ex() %>% check_object("tune_ctrl") %>% check_equal()
    ex() %>% check_object("res_desc") %>% check_equal()
    ex() %>% check_object("df_tune") %>% check_column("mtry") %>% check_equal()
    ex() %>% check_object("df_tune") %>% check_column("min.node.size") %>% check_equal()
    ex() %>% check_object("df_tune") %>% check_column("auc.test.mean") %>% check_equal()
  })
  if (! is.null(msg))
    return(msg)

  return(list(message = "Great job! :)", correct = TRUE, location = "append"))
}
```

```{r tuning-exec, exercise=TRUE, exercise.lines=25, exercise.timelimit=300L, exercise.checker=tuningChecker}
library(ggplot2)

learner =
param_set =
tune_ctrl =
res_desc =

set.seed(314)
tune_res = tuneParams(...)
tune_res

# Visualization
tune_data = generateHyperParsEffectData(tune_res)
ggplot(data = tune_data$data, aes(x = mtry, y = min.node.size, color = auc.test.mean)) + geom_point()
```

```{r tuning-exec-hint-1}
# Use the objects defined previously
learner = makeLearner("classif.ranger", predict.type = "prob")

param_set = makeParamSet(
  makeIntegerParam("mtry", lower = 1, upper = 30),
  makeIntegerParam("min.node.size", lower = 1, upper = 50)
)

tune_ctrl = makeTuneControlRandom(maxit = 100L)
res_desc = makeResampleDesc("CV", iters = 3L)
```

```{r tuning-exec-hint-2}
# Use the function 'tuneParams()' to finally conduct tuning
?tuneParams()
```

```{r tuning-exec-hint-3}
# Set the learner ('learner'), task ('task'), resampling strategy ('resampling'),
# parameter set ('par.set'), tuning strategy ('control'), and the measure ('measrues')
# to optimize in 'tuneParams()'
tune_res = tuneParams(learner, task = sonar.task, resampling = res_desc,
  par.set = param_set, control = tune_ctrl, measures = auc, show.info = FALSE)
```

```{r tuning-exec-solution}
library(ggplot2)

learner = makeLearner("classif.ranger", predict.type = "prob")

param_set = makeParamSet(
  makeIntegerParam("mtry", lower = 1, upper = 30),
  makeIntegerParam("min.node.size", lower = 1, upper = 50)
)

tune_ctrl = makeTuneControlRandom(maxit = 100L)
res_desc = makeResampleDesc("CV", iters = 3L)

set.seed(314)
tune_res = tuneParams(learner, task = sonar.task, resampling = res_desc,
  par.set = param_set, control = tune_ctrl, measures = auc, show.info = FALSE)
tune_res

tune_data = generateHyperParsEffectData(tune_res)
ggplot(data = tune_data$data, aes(x = mtry, y = min.node.size, color = auc.test.mean)) + geom_point()
```

```{r tuning-exec-check}
library(ggplot2)

learner = makeLearner("classif.ranger", predict.type = "prob")

param_set = makeParamSet(
  makeIntegerParam("mtry", lower = 1, upper = 30),
  makeIntegerParam("min.node.size", lower = 1, upper = 50)
)

tune_ctrl = makeTuneControlRandom(maxit = 100L)
res_desc = makeResampleDesc("CV", iters = 3L)

set.seed(314)
tune_res = tuneParams(learner, task = sonar.task, resampling = res_desc,
  par.set = param_set, control = tune_ctrl, measures = auc, show.info = FALSE)
tune_res

tune_data = generateHyperParsEffectData(tune_res)
ggplot(data = tune_data$data, aes(x = mtry, y = min.node.size, color = auc.test.mean)) + geom_point()
```
