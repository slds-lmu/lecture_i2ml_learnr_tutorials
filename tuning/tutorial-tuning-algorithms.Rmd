## Algorithms

### Study Goals

*Theoretical (T)*

- Get familiar with different tuning strategies
- Know the advantages and disadvantages of the tuning strategies
- Learn why and how nested resampling is done

*Practical (P)*

- Learn how to define the search space for parameter tuning.
- Conduct tuning for specific learner

### Preparation

1.  *(T)* Watch the following videos:
    <center>
    ![Basic Techniques](https://youtu.be/A1cx4FkS0lw){width="75%"}
    ![Nested Resampling Motivation](){width="75%"}
    ![Training - Validation - Testing](https://youtu.be/8LdpxLyH34c){width="75%"}
    ![Nested Resampling](https://youtu.be/-d338rc076s){width="75%"}
    </center>

1. *(P)* Read the tutorial about [tuning](https://mlr3book.mlr-org.com/tuning.html)

### Exercises

#### *(T)* Quiz

```{r tuning-quiz-algorithms, echo=FALSE}
question("Which statements are true?",
  # answer("Tuning means optimizing hyperparameters.", correct = TRUE),
  # answer("Doing tuning well is hard; nested resampling can help.", correct = TRUE),
  # answer("Good tuning is crucial to achieve good performance for all ML algorithms."),
  # # answer("Parallelization is trivial for tuning.", correct = TRUE),
  # answer("Tuning optimizes the inner loss."),
  answer("How well tuning works depends on the learner and the impact of the hyperparameters on that learner.", correct = TRUE),
  answer("Grid search often works better than random search."),
  answer("Grid search scales exponentially with the dimension of the parameter space.", correct = TRUE),
  answer("Grid search evaluates many points from the parameter space that aren't of interest.", correct = TRUE),
  answer("Random search works often better due to it's better exploration of the hyperparameter space.", correct = TRUE),
  answer("Random search scales very well with the dimension of the hyperparameter space."),
  answer("Random search as well as grid search has the problem of discretization.")
)
```

#### *(P)* The sonar task

In this hands-on we want to tune a random forest using the `classif.ranger` learner. The goal is to find hyperparameter values for `mtry` and `min.node.size` to achieve a high AUC. The task we are using is the task `"sonar"`. Taking a look at the help page of the task (`??mlbench::Sonar`) gives a nice description:

> This is the data set used by Gorman and Sejnowski in their study of the classification of sonar signals using a neural network [1]. The task is to train a network to discriminate between sonar signals bounced off a metal cylinder and those bounced off a roughly cylindrical rock.
>
> Each pattern is a set of 60 numbers in the range 0.0 to 1.0. Each number represents the energy within a particular frequency band, integrated over a certain period of time. The integration aperture for higher frequencies occur later in time, since these frequencies are transmitted later during the chirp.
>
> The label associated with each record contains the letter "R" if the object is a rock and "M" if it is a mine (metal cylinder). The numbers in the labels are in increasing order of aspect angle, but they do not encode the angle directly.

```{r sonar-task, exercise=TRUE}
tsk("sonar")
```

#### *(P)* Define the learner and parameter set

As mentioned above, we want to find good values for `mtry` and `min.node.size`. Therefore, define a hyperparameter space using `ParamSet$new()`. Before defining the parameter set define the `ranger` learner (do also note that we want to optimize for the AUC which requires the estimation of probabilities):

```{r ranger-learner, exercise=TRUE, exercise.checker=learnerChecker("learner")}
learner <- lrn(...)
```

```{r ranger-learner-solution}
# Use the 'ranger.learner' with 'predict_type = "prob"' to prepare the learner for predicting probabilities
learner <- lrn("classif.ranger", predict_type = "prob")
```

```{r ranger-learner-check}
learner <- lrn("classif.ranger", predict_type = "prob")
```

Now, define the parameter set with:

- $\texttt{mtry} \in [1,30]$
- $\texttt{min.node.size} \in [1,50]$

```{r tuning-ps, exercise=TRUE, exercise.checker=createChecker("param_set")}
param_set <- ParamSet$new(list(
   ..., # mtry
   ... # min.node.size
))
```

```{r tuning-ps-hint-1}
# To see all available parameter, it's type, the range, and if it is tuneable or not
# with 'learner$param_set'
learner$param_set
```

```{r tuning-ps-solution}
param_set <- ParamSet$new(list(
   ParamInt$new("mtry", lower = 1L, upper = 30L),
   ParamInt$new("min.node.size", lower = 1L, upper = 50L)
))
```

```{r tuning-ps-check}
param_set <- ParamSet$new(list(
   ParamInt$new("mtry", lower = 1L, upper = 30L), 
   ParamInt$new("min.node.size", lower = 1L, upper = 50L)
))
```

#### *(P)* Choose the general tuning scenario and resampling strategy

Set 50 iterations as the stopping criterion for the tuner. For the resampling of each value we use a 3-fold cross-validation. As the measure optimized by the tuner use the AUC:

```{r tuning-strategy, exercise=TRUE, exercise.checker=createChecker(c("instance"))}
learner <- ...
param_set <- ...
res_desc <- ...
evals50 <- term(...)
  
instance <- TuningInstance$new(
  task = ...,
  learner = ...,
  resampling = ...,
  measures = ...,
  param_set = ...,
  terminator = ...
)
```

```{r tuning-strategy-hint-1}
# Use the objects as defined before
learner <- lrn("classif.ranger", predict_type = "prob")
param_set <- ParamSet$new(list(
   ParamInt$new("mtry", lower = 1L, upper = 30L), 
   ParamInt$new("min.node.size", lower = 1L, upper = 50L)
))
```

```{r tuning-strategy-hint-2}
# As resampling description use 3-fold cross-validation
res_desc <- rsmp("cv", folds = 3L)
```

```{r tuning-strategy-hint-3}
# to specify the number of iterations, use the 'evals' option and set n_evals to 50
evals50 <- term("evals", n_evals = 50L)
```

```{r tuning-strategy-hint-4}
# The measure we want to optimize is the AUC
msr("classif.auc")
```

```{r tuning-strategy-solution}
learner <- lrn("classif.ranger", predict_type = "prob")
param_set <- ParamSet$new(list(
   ParamInt$new("mtry", lower = 1L, upper = 30L), 
   ParamInt$new("min.node.size", lower = 1L, upper = 50L)
))
res_desc <- rsmp("cv", folds = 3L)
evals50 <- term("evals", n_evals = 50L)
  
instance <- TuningInstance$new(
  task = tsk("sonar"),
  learner = learner,
  resampling = res_desc,
  measures = msr("classif.auc"),
  param_set = param_set,
  terminator = evals50
)
```

```{r tuning-strategy-check}
learner <- lrn("classif.ranger", predict_type = "prob")
param_set <- ParamSet$new(list(
   ParamInt$new("mtry", lower = 1L, upper = 30L), 
   ParamInt$new("min.node.size", lower = 1L, upper = 50L)
))
res_desc <- rsmp("cv", folds = 3L)
evals50 <- term("evals", n_evals = 50L)
  
instance <- TuningInstance$new(
  task = tsk("sonar"),
  learner = learner,
  resampling = res_desc,
  measures = msr("classif.auc"),
  param_set = param_set,
  terminator = evals50
)
```

#### *(P)* Run and visualize the tuning

Finally, create a random search tuner und use it to tune the tuning scenario `instance` with `tune()`. **Note** that the tuning could take some time to finish.

```{r, include=FALSE}
tuningChecker = function (label, user_code, check_code, envir_result, evaluate_result, ...)
{
  add_code = "
    res = instance$archive()
  "

  setup_state(sol_code = paste0(check_code, add_code), stu_code = paste0(user_code, add_code))

  msg = errorToMessage(expr = {
    ex() %>% check_object("instance") %>% check_column("param_set") %>% check_equal()
    ex() %>% check_object("instance") %>% check_column("result") %>% check_equal()
    ex() %>% check_object("res") %>% check_column("params") %>% check_equal()
  })
  if (! is.null(msg))
    return(msg)

  return(list(message = "Great job! :)", correct = TRUE, location = "append"))
}
```

```{r tuning-exec, exercise=TRUE, exercise.lines=25, exercise.timelimit=300L, exercise.checker=tuningChecker}
library(ggplot2)

learner <- ...
param_set <- ...
res_desc <- ...
evals50 <- term(...)
  
instance <- TuningInstance$new(
  task = ...,
  learner = ...,
  resampling = ...,
  measures = ...,
  param_set = ...,
  terminator = ...
)

set.seed(314)
tuner <- tnr(...)
tuner$
# 'Best' model
instance$result
  
# Visualization
res <- instance$archive()
par_combs <- as.data.frame(matrix(unlist(res$params), nrow = length(res$params), byrow = T))
colnames(par_combs) <- names(res$params[[1]])
par_combs$auc <- res$classif.auc
ggplot(data = par_combs, aes(x = mtry, y = min.node.size, color = auc)) + geom_point()
```

```{r tuning-exec-hint-1}
# Use the objects defined previously
learner <- lrn("classif.ranger", predict_type = "prob")
param_set <- ParamSet$new(list(
   ParamInt$new("mtry", lower = 1L, upper = 30L), 
   ParamInt$new("min.node.size", lower = 1L, upper = 50L)
))
res_desc <- rsmp("cv", folds = 3L)
evals50 <- term("evals", n_evals = 50L)
  
instance <- TuningInstance$new(
  task = tsk("sonar"),
  learner = learner,
  resampling = res_desc,
  measures = msr("classif.auc"),
  param_set = param_set,
  terminator = evals50
)
```

```{r tuning-exec-hint-2}
# Create a tuner with random search strategy
tuner <- tnr("random_search")
```

```{r tuning-exec-hint-3}
# Tune the tuning scenario `instance`
tuner$tune(instance)
```

```{r tuning-exec-solution}
library(ggplot2)

learner <- lrn("classif.ranger", predict_type = "prob")
param_set <- ParamSet$new(list(
   ParamInt$new("mtry", lower = 1L, upper = 30L), 
   ParamInt$new("min.node.size", lower = 1L, upper = 50L)
))
res_desc <- rsmp("cv", folds = 3L)
evals50 <- term("evals", n_evals = 50L)
  
instance <- TuningInstance$new(
  task = tsk("sonar"),
  learner = learner,
  resampling = res_desc,
  measures = msr("classif.auc"),
  param_set = param_set,
  terminator = evals50
)

set.seed(314)
tuner <- tnr("random_search")
tuner$tune(instance)
# 'Best' model
instance$result

# Visualization
res <- instance$archive()
par_combs <- as.data.frame(matrix(unlist(res$params), nrow = length(res$params), byrow = T))
colnames(par_combs) <- names(res$params[[1]])
par_combs$auc <- res$classif.auc
ggplot(data = par_combs, aes(x = mtry, y = min.node.size, color = auc)) + geom_point()
```

```{r tuning-exec-check}
learner <- lrn("classif.ranger", predict_type = "prob")
param_set <- ParamSet$new(list(
   ParamInt$new("mtry", lower = 1L, upper = 30L), 
   ParamInt$new("min.node.size", lower = 1L, upper = 50L)
))
res_desc <- rsmp("cv", folds = 3L)
evals50 <- term("evals", n_evals = 50L)
  
instance <- TuningInstance$new(
  task = tsk("sonar"),
  learner = learner,
  resampling = res_desc,
  measures = msr("classif.auc"),
  param_set = param_set,
  terminator = evals50
)

set.seed(314)
tuner <- tnr("random_search")
tuner$tune(instance)
```
