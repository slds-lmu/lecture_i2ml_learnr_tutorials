## Logistic Regression

### Study Goals

*Theoretical (T)*

- Understand the way logistic regression works
- Learn how to transform model scores to model probabilities with the logistic function
- Get to know binary classification losses
- Understand the basic idea of multinomial regression

*Practical (P)*

- Be able to train a logistic regression with `R` and `mlr`
- Practice how to transform linear into (simple) non-linear classifiers

### Preparation

1.  *(T)* Watch the following video  (sorry, rather low volume...):
    <center>
    ![](https://www.youtube.com/watch?v=l3yBsf7Jq5E&t=0s&index=3&list=PLMyWaJl2LoXxjj3A-nctKtkG5xJqrdFMU){width="75%"}
    </center>

1.  *(P)* Make sure that you have understood how to define tasks and learners and how to train a learner in `mlr`.

### Exercises

#### *(T)* Quiz

```{r logi-quiz1, echo=FALSE}
question("Which statements are true?",
  answer("Logistic regression can be fitted by maximum likelihood with numerical optimization or solved analytically."),
  answer("The softmax function is a generalization of the logistic function.", correct = TRUE),
  answer("In logistic regression, the parameter vector $\\theta$ that maximizes the model's likelihood is identical to the one minimizing its empirical risk.", correct = TRUE),
  answer("In softmax regression, we often set $\\theta_{g} = (0, \\dots, 0)^T$ and only optimize the other $\\theta_{k}$ ($k\\neq g$) to avoid overparameterization.", correct = TRUE)
)
```

#### *(P)* Training a logistic regression with `mlr`

For this exercise, take a look at the `titanic_train` dataset from the `titanic` package. Just keep the features `Survived`, `Age`, and `Fare`. Remove all observations with missing values `NA`s:

```{r titanic-data, exercise=TRUE, exercise.lines=5, exercise.checker=createChecker("my_titanic")}
library(titanic)

my_titanic =
```

```{r titanic-data-hint-1}
# Install and library the titanic package or use the namespace titanic to load the 'titanic_train' dataset
titanic::titanic_train
```

```{r titanic-data-hint-2}
# Use 'na.omit' to remove all observations that contains missing values
na.omit(...)
```

```{r titanic-data-solution}
na.omit(titanic::titanic_train[, c("Survived", "Age", "Fare")])
```

```{r titanic-data-check}
my_titanic = na.omit(titanic::titanic_train[, c("Survived", "Age", "Fare")])
```

Now define a classification task `task_titanic` on that dataset with target `Survived`, define a logistic regression learner, and train that learner:

```{r, include=FALSE}
my_titanic = na.omit(titanic::titanic_train[, c("Survived", "Age", "Fare")])
```

```{r titanic-train, exercise=TRUE, exercise.lines=8, exercise.checker=createChecker(c("task_titanic", "learner_logreg", "model_titanic"))}
task_titanic =
learner_logreg =
model_titanic =
```

```{r titanic-train-hint-1}
# We have to define a classification task since we are classifying if a passenger survived or not we
task_titanic = makeClassifTask(data = my_titanic, target = "Survived")
```

```{r titanic-train-hint-2}
# The learner we are looking for is 'classif.logreg'
learner_logreg = makeLearner("classif.logreg")
```

```{r titanic-train-hint-3}
# Finally we have to train the learner
model_titanic = train(learner_logreg, task_titanic)
```

```{r titanic-train-solution}
task_titanic = makeClassifTask(data = my_titanic, target = "Survived")
learner_logreg = makeLearner("classif.logreg")
model_titanic = train(learner_logreg, task_titanic)
```


```{r titanic-train-check}
task_titanic = makeClassifTask(data = my_titanic, target = "Survived")
learner_logreg = makeLearner("classif.logreg")
model_titanic = train(learner_logreg, task_titanic)
```

Finally, visualize the model with `plotLearnerPrediction()`:

```{r, include=FALSE}
my_titanic = na.omit(titanic::titanic_train[, c("Survived", "Age", "Fare")])
task_titanic = makeClassifTask(data = my_titanic, target = "Survived")
learner_logreg = makeLearner("classif.logreg")
```

```{r titanic-viz, exercise=TRUE}
plotLearnerPrediction(learner = ..., task = ...)
```

#### *(P)* Training a logistic regression with non-linear decision boundaries

```{r, include=FALSE}
polynomialTrafo = function (data, feature, degree) {
  feature_idx = which(feature == names(data))
  df_poly = as.data.frame(poly(data[[feature]], degree))
  names(df_poly) = paste0(feature, ".poly", seq_len(degree))
  return(cbind(data[, -feature_idx, drop = FALSE], df_poly))
}

my_titanic = na.omit(titanic::titanic_train[, c("Survived", "Age", "Fare")])
```

The next demonstration shows how to include the features `Age` and `Fare` as polynomials and the effect on the decision boundary. As mentioned in the video, it is possible to transform a linear classifier into a non-linear classifier by just mapping features into a higher dimensional feature space (feature map):

```{r titanic-non-linear, exercise=TRUE}
library(ggplot2)

# Change degree and threshold here:
degree = 3
threshold = 0.5

# You can leave this code as it is, just vary degree and threshold above
# and see how the prediction surface change:
task_data = polynomialTrafo(my_titanic, "Age", degree)
task_data = polynomialTrafo(task_data, "Fare", degree)

titanic_task = makeClassifTask(data = task_data, target = "Survived")
titanic_learner = makeLearner("classif.logreg", predict.type = "prob")
logreg_model = train(titanic_learner, titanic_task)

age_fare_pred = expand.grid(
  Age = seq(min(my_titanic$Age), max(my_titanic$Age), length.out = 100),
  Fare = seq(min(my_titanic$Fare), max(my_titanic$Fare), length.out = 100)
)
pred_data = cbind(
  polynomialTrafo(age_fare_pred[,"Age",drop=FALSE], "Age", degree),
  polynomialTrafo(age_fare_pred[,"Fare",drop=FALSE], "Fare", degree)
)
pred = setThreshold(predict(logreg_model, newdata = pred_data), threshold)
plot_data = data.frame(pred = pred$data$response, 
  Age = age_fare_pred$Age, Fare = age_fare_pred$Fare)
pred_task = setThreshold(predict(logreg_model, titanic_task), threshold)$data$response
my_titanic$Classified = ifelse(pred_task == my_titanic$Survived, "correct", "wrong")
ggplot() + 
  geom_point(data = plot_data, aes(x = Age, y = Fare, color = as.factor(pred)), alpha = 0.2, show.legend = FALSE) +
  geom_point(data = my_titanic, aes(x = Age, y = Fare, color = as.factor(Survived), shape = Classified), size = 2, show.legend = FALSE)
```

