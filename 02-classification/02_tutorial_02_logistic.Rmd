## Logistic Regression

### Study Goals

*Theoretical (T)*

- Understand the way logistic regression works
- Learn how to transform model scores to model probabilities with the logistic function
- Get to know binary classification losses
- Understand the basic idea of multinomial regression

*Practical (P)*

- Be able to train a logistic regression with `R` and `mlr`
- Practice how to transform linear into (simple) non-linear classifiers

### Preparation

1.  *(T)* Watch the following video  (sorry, rather low volume...):
    <center>
    ![](https://youtu.be/VUa49fo82q4){width="75%"}
    </center>
    <a href="https://github.com/compstat-lmu/lecture_i2ml/blob/master/slides-pdf/slides-logistic.pdf" target="_blank">Slideset</a>

1.  *(P)* Make sure that you have understood how to define tasks and learners and how to train a learner in `mlr`.

### Exercises

#### *(T)* Quiz

```{r logi-quiz1, echo=FALSE}
question("Which statements are true?",
  answer("Logistic regression can be fitted by maximum likelihood with numerical optimization or solved analytically."),
  answer("The softmax function is a generalization of the logistic function.", correct = TRUE),
  answer("In logistic regression, the parameter vector $\\theta$ that maximizes the model's likelihood is identical to the one minimizing its empirical risk.", correct = TRUE),
  answer("In softmax regression, we often set $\\theta_{g} = (0, \\dots, 0)^T$ and only optimize the other $\\theta_{k}$ ($k\\neq g$) to avoid overparameterization.", correct = TRUE)
)
```

#### *(P)* Training a logistic regression with `mlr`

For this exercise, take a look at the `titanic_train` dataset from the `titanic` package. Just keep the features `Survived`, `Age`, and `Fare`. Remove all observations with missing values `NA`s:

```{r, include=FALSE}
titanicChecker = function (label, user_code, check_code, envir_result, evaluate_result, ...)
{
  setup_state(sol_code = check_code, stu_code = user_code)

  msg = errorToMessage(expr = {
    ex() %>% check_object("my_titanic", undefined_msg = "No object \'my_titanic\' found! Please name your data frame as \'my_titanic\'.")
    ex() %>% check_object("my_titanic") %>% check_column("Survived") %>% check_equal()
    ex() %>% check_object("my_titanic") %>% check_column("Age") %>% check_equal()
    ex() %>% check_object("my_titanic") %>% check_column("Fare") %>% check_equal()
  })
  if (! is.null(msg))
    return(msg)

  return(list(message = "Great job! :)", correct = TRUE, location = "append"))
}
```

```{r titanic-data, exercise=TRUE, exercise.lines=5, exercise.checker=titanicChecker}
library(titanic)

my_titanic =
```

```{r titanic-data-hint-1}
# Install and library the titanic package or use the namespace titanic to load the 'titanic_train' dataset
titanic::titanic_train
```

```{r titanic-data-hint-2}
# Use 'na.omit' to remove all observations that contains missing values
na.omit(...)
```

```{r titanic-data-solution}
my_titanic = na.omit(titanic::titanic_train[, c("Survived", "Age", "Fare")])
```

```{r titanic-data-check}
my_titanic = na.omit(titanic::titanic_train[, c("Survived", "Age", "Fare")])
```

Now define a classification task `task_titanic` on that dataset with target `Survived`, define a logistic regression learner, and train that learner:

```{r, include=FALSE}
my_titanic = na.omit(titanic::titanic_train[, c("Survived", "Age", "Fare")])
```

```{r titanic-train, exercise=TRUE, exercise.lines=8, exercise.checker=modelChecker("model_titanic")}
my_titanic =
task_titanic =
learner_logreg =
model_titanic =
```

```{r titanic-train-hint-1}
# We have to define a classification task since we are classifying if a passenger survived or not we
task_titanic = makeClassifTask(data = my_titanic, target = "Survived")
```

```{r titanic-train-hint-2}
# The learner we are looking for is 'classif.logreg'
learner_logreg = makeLearner("classif.logreg")
```

```{r titanic-train-hint-3}
# Finally we have to train the learner
model_titanic = train(learner_logreg, task_titanic)
```

```{r titanic-train-solution}
my_titanic = na.omit(titanic::titanic_train[, c("Survived", "Age", "Fare")])
task_titanic = makeClassifTask(data = my_titanic, target = "Survived")
learner_logreg = makeLearner("classif.logreg")
model_titanic = train(learner_logreg, task_titanic)
```


```{r titanic-train-check}
my_titanic = na.omit(titanic::titanic_train[, c("Survived", "Age", "Fare")])
task_titanic = makeClassifTask(data = my_titanic, target = "Survived")
learner_logreg = makeLearner("classif.logreg")
model_titanic = train(learner_logreg, task_titanic)
```

Finally, visualize the model with `plotLearnerPrediction()`:

```{r titanic-viz, exercise=TRUE}
my_titanic =
task_titanic =
learner_logreg =

plotLearnerPrediction(learner = ..., task = ...)
```

```{r titanic-viz-solution}
my_titanic = na.omit(titanic::titanic_train[, c("Survived", "Age", "Fare")])
task_titanic = makeClassifTask(data = my_titanic, target = "Survived")
learner_logreg = makeLearner("classif.logreg")

plotLearnerPrediction(learner = learner_logreg, task = task_titanic)
```

#### *(P)* Training a logistic regression with non-linear decision boundaries

```{r titanic-non-linear-setup}
polynomialTrafo = function (data, feature, degree) {
  feature_idx = which(feature == names(data))
  # df_poly = as.data.frame(poly(data[[feature]], degree))

  # I think this is ok here, the boundaries looks so crappy with poly ...
  df_poly = as.data.frame(do.call(cbind, lapply(seq_len(degree), function (d) data[[feature]]^d)))
  names(df_poly) = paste0(feature, ".poly", seq_len(degree))
  return(cbind(data[, -feature_idx, drop = FALSE], df_poly))
}
```

The next demonstration shows how to include the features `Age` and `Fare` as polynomials and the effect on the decision boundary. As mentioned in the video, it is possible to transform a linear classifier into a non-linear classifier by just mapping features into a higher dimensional feature space (feature map):

```{r titanic-non-linear, exercise.lines=6, exercise=TRUE}
library(ggplot2)

# Change degree and threshold here:
degree = 3
threshold = 0.5

# You can leave this code as it is, just vary degree and threshold above
# and see how the prediction surface change:
my_titanic = na.omit(titanic::titanic_train[, c("Survived", "Age", "Fare")])

task_data = polynomialTrafo(my_titanic, "Age", degree)
task_data = polynomialTrafo(task_data, "Fare", degree)

titanic_task = makeClassifTask(data = task_data, target = "Survived")
titanic_learner = makeLearner("classif.logreg", predict.type = "prob")
logreg_model = train(titanic_learner, titanic_task)

logreg_train_predictions = setThreshold(predict(logreg_model, titanic_task), threshold)$data$response
my_titanic$correct_prediction = logreg_train_predictions == my_titanic$Survived

age_fare_grid = expand.grid(
  Age = seq(min(my_titanic$Age), max(my_titanic$Age), length.out = 100),
  Fare = seq(min(my_titanic$Fare), max(my_titanic$Fare), length.out = 100)
)
age_fare_grid_polys = cbind(
  polynomialTrafo(age_fare_grid[,"Age",drop=FALSE], "Age", degree),
  polynomialTrafo(age_fare_grid[,"Fare",drop=FALSE], "Fare", degree)
)
age_fare_grid_polys_prediction = setThreshold(predict(logreg_model, newdata = age_fare_grid_polys), threshold)
age_fare_grid$pred = age_fare_grid_polys_prediction$data$response
age_fare_grid$probs = age_fare_grid_polys_prediction$data$prob.0

ggplot() +
  geom_point(data = age_fare_grid, aes(x = Age, y = Fare, color = as.factor(pred), alpha = ifelse(probs < 0.5, 1 - probs, probs)),
    show.legend = FALSE) +
  geom_point(data = subset(my_titanic, ! correct_prediction),
           color = "white", size = 3, mapping = aes(x = Age, y = Fare, shape = as.factor(Survived)),
           show.legend = FALSE) +
  geom_point(data = subset(my_titanic, ! correct_prediction),
           color = "black", size = 3, mapping = aes(x = Age, y = Fare), show.legend = FALSE,
           shape = c(1,2)[as.factor(subset(my_titanic, ! correct_prediction)$Survived)]) +
  geom_point(data = my_titanic, aes(x = Age, y = Fare, shape = as.factor(Survived)), size = 2) +
  scale_shape_manual(values = c(16, 17), name = "Survived")
```

