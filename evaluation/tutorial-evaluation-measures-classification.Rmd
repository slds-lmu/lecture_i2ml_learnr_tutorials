## Evaluation Measures for Classification

### Study Goals

*Theoretical (T)*

- Get familiar with simple performance measures for classification
- Get familiar with confusion matrix and ROC
- Know how to evaluate unbalanced binary classification problems

*Practical (P)*

- Calculation of confusion matrices
- Plotting of the ROC
- Calculating the AUC


### Preparation

1.  *(T)* Watch the following videos:
    <center>
    ![](https://youtu.be/bHwUwrbCHEU){width="75%"}
    </center>
    <!-- <a href="https://github.com/compstat-lmu/lecture_i2ml/blob/master/slides-pdf/slides-roc-1.pdf" target="_blank">Slideset Part 1</a> -->
    <center>
    ![](https://youtu.be/BH4oCliBzZI){width="75%"}
    </center>
    <!-- <a href="https://github.com/compstat-lmu/lecture_i2ml/blob/master/slides-pdf/slides-roc-1.pdf" target="_blank">Slideset Part 1</a> -->
    <center>
    ![](https://youtu.be/m5We8ITYEVk){width="75%"}
    </center>
    <!-- <a href="https://github.com/compstat-lmu/lecture_i2ml/blob/master/slides-pdf/slides-roc-2.pdf" target="_blank">Slideset Part 2</a> -->

1. *(P)* Read the `mlr` tutorial about [roc curves](https://mlr.mlr-org.com/articles/tutorial/roc_analysis.html)
1. *(P)* You should have done the tutorial on resampling

### Exercises


#### *(T)* Quiz

```{r measures-classification-quiz, echo=FALSE}
question("Which statements are true?",
    answer("Logistic regression minimizes the binomial loss.", correct = TRUE),
    answer("The Brier score is like the MSE just with probabilities.", correct = TRUE),
    answer("The log-loss punishes being very wrong less than the Brier score."),
    answer("Accuracy and mean classification error are calculated using the predicted probabilities."),
    answer("The confusion matrix tabulates the true against predicted classes.", correct = TRUE),
    answer("A misclassification error rate of 0.5% is always great.")
)
```


#### *(T)* Quiz

```{r ROC-quiz, echo=FALSE}
question("Which statements are true?",
  answer("If the proportion of positive to negative instances in the training data changes, the ROC curve will not change."),
  answer("If the proportion of positive to negative instances in the test data changes, the ROC curve will not change.", correct = TRUE),
  answer("Several evaluation metrics can be derived from a confusion matrix.", correct = TRUE),
  answer("The area under the ROC curve is called AUC.", correct = TRUE),
  answer("AUC = 0 means that the model is optimal.")
)
```

#### *(P)* The spam dataset

Make yourself familiar with the spam task (`?kernlab::spam`):

> A data set collected at Hewlett-Packard Labs, that classifies 4601 e-mails as spam or non-spam. In addition to this class label there are 57 variables indicating the frequency of certain words and characters in the e-mail.

The task is predefined in `mlr` as `spam.task`:

```{r spam-task, exercise=TRUE}
spam.task
head(spam.task$env$data)
?kernlab::spam
```

#### *(P)* A first model

1. As a first approach, we want to train a logistic regression on the whole task. Therefore, define the model and train it. Set the `predict.type` of the learner to `"prob"`:

```{r spam-logreg-train, exercise=TRUE, exercise.lines=5, exercise.checker=modelChecker("model_logreg", TRUE)}
learner =
model_logreg = train(learner = ..., task = ...)
```

```{r spam-logreg-train-hint-1}
# Use 'classif.logreg' as learner with 'predict.type = prob'
learner = makeLearner("classif.logreg", predict.type = "prob")
```

```{r spam-logreg-train-hint-2}
# Use 'spam.task' as task
task = spam.task
```

```{r spam-logreg-train-solution}
learner = makeLearner("classif.logreg", predict.type = "prob")
model_logreg = train(learner = learner, task = spam.task)
```

```{r spam-logreg-train-check}
learner = makeLearner("classif.logreg", predict.type = "prob")
model_logreg = train(learner = learner, task = spam.task)
```

2. Calculate the prediction on the whole task with `predict()`:

```{r, include=FALSE}
predCheck = function (label, user_code, check_code, envir_result, evaluate_result, ...)
{
  setup_state(sol_code = check_code, stu_code = user_code)

  msg = errorToMessage(expr = {
    ex() %>% check_object("model_prediction") %>% check_element("data") %>% check_equal()
  })
  if (! is.null(msg))
    return(msg)

  return(list(message = "Great job! :)", correct = TRUE, location = "append"))
}
```

```{r spam-logreg-pred, exercise=TRUE, exercise.lines=5, exercise.checker=predCheck}
learner =
moel_logreg =
model_prediction = predict(...)
```

```{r spam-logreg-pred-hint-1}
# Define the model as previously
learner = makeLearner("classif.logreg", predict.type = "prob")
model_logreg = train(learner = learner, task = spam.task)
```

```{r spam-logreg-pred-hint-2}
# Pass the model and task to predict()
model_prediction = predict(model_logreg, spam.task)
```

```{r spam-logreg-pred-solution}
learner = makeLearner("classif.logreg", predict.type = "prob")
model_logreg = train(learner = learner, task = spam.task)
model_prediction = predict(model_logreg, spam.task)
```

```{r spam-logreg-pred-check}
learner = makeLearner("classif.logreg", predict.type = "prob")
model_logreg = train(learner = learner, task = spam.task)
model_prediction = predict(model_logreg, spam.task)
```

3. Calculate the confusion matrix for the `model_prediction` (extra: use `setThreshold()` to vary the threshold within the `calculateConfusionMatrix()` function):

```{r conf-mat, exercise=TRUE}
learner = makeLearner("classif.logreg", predict.type = "prob")
model_logreg = train(learner = learner, task = spam.task)
model_prediction = predict(model_logreg, spam.task)

calculateConfusionMatrix(...)
calculateConfusionMatrix(setThreshold(..., 0.2))
```

```{r conf-mat-solution}
learner = makeLearner("classif.logreg", predict.type = "prob")
model_logreg = train(learner = learner, task = spam.task)
model_prediction = predict(model_logreg, spam.task)

calculateConfusionMatrix(model_prediction)
calculateConfusionMatrix(setThreshold(model_prediction, 0.2))
```

4. Finally, generate the threshold vs. performance data (with `gneerateThreshVsPerfData()`) that are needed to plot the ROC (with `plotROCCurves()`). Do also calculate the AUC and mmce with `performance()` (extra: use `setThreshold()` to vary the threshold within the `performance()` function):

```{r, include=FALSE}
threshCheck = function (label, user_code, check_code, envir_result, evaluate_result, ...)
{
  code_ext = "
  thresh_data_names = sort(names(thresh_vs_perf_data$data)[c(1,2)])
  "

  setup_state(sol_code = paste0(check_code, code_ext), stu_code = paste0(user_code, code_ext))

  msg = errorToMessage(expr = {
    ex() %>% check_object("thresh_vs_perf_data") %>% check_element("data") %>% check_column("fpr") %>% check_equal()
    ex() %>% check_object("thresh_vs_perf_data") %>% check_element("data") %>% check_column("tpr") %>% check_equal()
    ex() %>% check_object("thresh_data_names") %>% check_equal("Make sure that the first two measures are fpr and tpr!")
  })
  if (! is.null(msg))
    return(msg)

  return(list(message = "Great job! :)", correct = TRUE, location = "append"))
}
```

```{r roc-auc, exercise=TRUE, exercise.checker=threshCheck}
learner =
moel_logreg =
model_prediction = predict(...)

thresh_vs_perf_data = generateThreshVsPerfData(..., measures = ...)
plotROCCurves(thresh_vs_perf_data)
performance(pred = ..., measures = ...)
```

```{r roc-auc-hint-1}
# Use the previously defined objects
learner = makeLearner("classif.logreg", predict.type = "prob")
model_logreg = train(learner = learner, task = spam.task)
model_prediction = predict(model_logreg, spam.task)
```

```{r roc-auc-hint-2}
# The ROC is calculated by plotting the false positive rate vs. true positive rate.
# To extract them from the prediction object use
thresh_vs_perf_data = generateThreshVsPerfData(model_prediction, measures = list(fpr, tpr))
```

```{r roc-auc-hint-3}
# The ROC can be easily plotted with
plotROCCurves(thresh_vs_perf_data)
```

```{r roc-auc-hint-4}
# To calculate the performance on a prediction object use the 'performance()' function
performance(pred = model_prediction, measures = list(auc, mmce))
```

```{r roc-auc-hint-5}
# To calculate prediction based on another threshold use `setThreshold()` on the prediction object:
setThreshold(model_prediction, 0.2)
```

```{r roc-auc-solution}
learner = makeLearner("classif.logreg", predict.type = "prob")
model_logreg = train(learner = learner, task = spam.task)
model_prediction = predict(model_logreg, spam.task)

thresh_vs_perf_data = generateThreshVsPerfData(model_prediction, measures = list(fpr, tpr))
plotROCCurves(thresh_vs_perf_data)
performance(pred = model_prediction, measures = list(auc, mmce))
performance(pred = setThreshold(model_prediction, 0.2), measures = list(auc, mmce))
```

```{r roc-auc-check}
learner = makeLearner("classif.logreg", predict.type = "prob")
model_logreg = train(learner = learner, task = spam.task)
model_prediction = predict(model_logreg, spam.task)

thresh_vs_perf_data = generateThreshVsPerfData(model_prediction, measures = list(fpr, tpr))
plotROCCurves(thresh_vs_perf_data)
performance(pred = model_prediction, measures = list(auc, mmce))
performance(pred = setThreshold(model_prediction, 0.2), measures = list(auc, mmce))
```

```{r roc-auc-quiz, echo=FALSE}
question("Which statements are true?",
  answer("The AUC with about 97 % is very good.", correct = TRUE),
  answer("The model is able to classify 1619 out of 1813 correct as nonspams."),
  answer("Using the prediction of the train data is the ordinary and correct way of calculating the ROC."),
  answer("The calculation of the ROC should be done on a test set.", correct = TRUE),
  answer("The AUC is not effected by the threshold whereas the mmce is.", correct = TRUE)
)
```

#### *(P)* ROC and AUC on test data

Using just the train dataset for predictions leads to overoptimistic ROC and AUC estimations. In this section we use `resample()` to obtain predictions of the whole dataset obtained by

1. To get a correct ROC use resample to evaluate the learner with a 3-fold cross validation:

```{r, include=FALSE}
resampleChecker = function (label, user_code, check_code, envir_result, evaluate_result, ...)
{
  msg = learnerChecker("learner", TRUE)(label, user_code, check_code, envir_result, evaluate_result)
  if (! is.null(msg))
    return(msg)

  setup_state(sol_code = check_code, stu_code = user_code)

  msg = errorToMessage(expr = {
    ex() %>% check_object("res_desc") %>% check_equal()
    ex() %>% check_object("res") %>% check_element("measures.train") %>% check_equal()
    ex() %>% check_object("res") %>% check_element("measures.test") %>% check_equal()
  })
  if (! is.null(msg))
    return(msg)

  return(list(message = "Great job! :)", correct = TRUE, location = "append"))
}

```

```{r logreg-res, exercise=TRUE, exercise.checker=resampleChecker}
learner =

set.seed(123)
res = resample(learner = ..., task = ..., resampling = ...)
```

```{r logreg-res-hint-1}
# Use 'classif.logreg' as learner with 'predict.type = prob'
learner = makeLearner("classif.logreg", predict.type = "prob")
```

```{r logreg-res-hint-2}
# Use 'spam.task' as task
task = spam.task
```

```{r logreg-res-hint-3}
# Use a 3-fold cross validation for resampling
resampling = cv3
resampling = makeResampleDesc(method = "CV", iters = 3)
```

```{r logreg-res-solution}
learner = makeLearner("classif.logreg", predict.type = "prob")

set.seed(123)
res = resample(learner = learner, task = spam.task, resampling = cv3)
```

```{r logreg-res-check}
learner = makeLearner("classif.logreg", predict.type = "prob")

set.seed(123)
res = resample(learner = learner, task = spam.task, resampling = cv3)
```

2. The resample object has an element `pred`. This prediction object contains the test predictions of each fold, therefore we have test based predictions of each observation. Extract the object from the `res` object and store it:

```{r, include=FALSE}
predCheck = function (label, user_code, check_code, envir_result, evaluate_result, ...)
{
  setup_state(sol_code = check_code, stu_code = user_code)

  msg = errorToMessage(expr = {
    ex() %>% check_object("test_prediction") %>% check_element("data") %>% check_equal()
  })
  if (! is.null(msg))
    return(msg)

  return(list(message = "Great job! :)", correct = TRUE, location = "append"))
}
```

```{r test-pred-res, exercise=TRUE, exercise.checker=predCheck}
learner =

set.seed(123)
res = resample(learner = ..., task = ..., resampling = ...)

test_prediction =
```

```{r test-pred-res-hint-1}
# Use the objects defined previously
learner = makeLearner("classif.logreg", predict.type = "prob")

set.seed(123)
res = resample(learner = learner, task = spam.task, resampling = cv3)
```

```{r test-pred-res-hint-2}
# To access the test predictions of each fold select the 'data' element of res
test_prediction = res$pred
```

```{r test-pred-res-solution}
learner = makeLearner("classif.logreg", predict.type = "prob")

set.seed(123)
res = resample(learner = learner, task = spam.task, resampling = cv3)

test_prediction = res$pred
```

```{r test-pred-res-check}
learner = makeLearner("classif.logreg", predict.type = "prob")

set.seed(123)
res = resample(learner = learner, task = spam.task, resampling = cv3)

test_prediction = res$pred
```

3. Finally, calculate the ROC and AUC based on the `test_prediction` object:

```{r roc-auc-res, exercise=TRUE, exercise.checker=threshCheck}
learner =

set.seed(123)
res = resample(learner = ..., task = ..., resampling = ...)

test_prediction =
thresh_vs_perf_data = generateThreshVsPerfData(..., measures = ...)
plotROCCurves(thresh_vs_perf_data)
performance(pred = test_prediction, measures = auc)
```

```{r roc-auc-res-hint-1}
# Use the objects defined previously
learner = makeLearner("classif.logreg", predict.type = "prob")

set.seed(123)
res = resample(learner = learner, task = spam.task, resampling = cv3)

test_prediction = res$pred
```

```{r roc-auc-res-hint-2}
# The ROC is calculated by plotting the false positive rate vs. true positive rate.
# To extract them from the prediction object use
thresh_vs_perf_data = generateThreshVsPerfData(test_prediction, measures = list(fpr, tpr))
```

```{r roc-auc-res-hint-3}
# The ROC can be easily plotted with
plotROCCurves(thresh_vs_perf_data)
```

```{r roc-auc-res-hint-4}
# To calculate the performance on a prediction object use the 'performance()' function
performance(pred = test_prediction, measures = list(auc, mmce))
```

```{r roc-auc-res-hint-5}
# To calculate prediction based on another threshold use `setThreshold()` on the prediction object:
setThreshold(test_prediction, 0.2)
```

```{r roc-auc-res-solution}
learner = makeLearner("classif.logreg", predict.type = "prob")

set.seed(123)
res = resample(learner = learner, task = spam.task, resampling = cv3)

test_prediction = res$pred
thresh_vs_perf_data = generateThreshVsPerfData(test_prediction, measures = list(fpr, tpr))
plotROCCurves(thresh_vs_perf_data)
performance(pred = test_prediction, measures = auc)
```

```{r roc-auc-res-check}
learner = makeLearner("classif.logreg", predict.type = "prob")

set.seed(123)
res = resample(learner = learner, task = spam.task, resampling = cv3)

test_prediction = res$pred
thresh_vs_perf_data = generateThreshVsPerfData(test_prediction, measures = list(fpr, tpr))
plotROCCurves(thresh_vs_perf_data)
performance(pred = test_prediction, measures = auc)
```
